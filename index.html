<!DOCTYPE html>
<!-- saved from url=(0042)https://cse.buffalo.edu/~jmeng2/index.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./page_files/analytics.js.download"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-3974203-1', 'auto'); ga('send', 'pageview');</script>
    
    <title>Homepage for Heng Fan</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="./stylefiles/global.css">
    <link rel="stylesheet" type="text/css" href="./stylefiles/navigation.css">
	<link rel="stylesheet" type="text/css" href="./stylefiles/home.css">
	<link rel="shortcut icon" href="./personal/favicon.ico" />
	<style>a{ TEXT-DECORATION:none}a:hover{TEXT-DECORATION:underline }</style>
	<!-- <style>a{ TEXT-DECORATION:none }</style>-->   <!-- change style for hyperlink -->
</head>

<body data-gr-c-s-loaded="true">

<div class="navi central_body">
    <a class="navi navi_active" href="./index.html">Home</a>
    <a class="navi" href="./publications.html">Selected Publications</a>
	<a class="navi" href="./group.html">Group</a>
    <a class="navi" href="./teaching.html">Teaching</a>
    <a class="navi" href="./service.html">Professional Activities</a>
	<!-- <a class="navi" href="./student.html">Student Supervision</a> -->
</div>

<div class="navi_bar"></div>  <!-- UNT Vision and AI Lab (VAIL) -->

<div class="central_body">

    <div class="photo_intro">
        <img src="./personal/hfan-2022.jpeg" alt="Heng Fan" class="photo" height="130">
        <div class="intro">
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="intro_name"><font size="4">Heng Fan</font></span>
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Assistant Professor
            <br> 
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Vision and Artificial Intelligence Lab (VAIL)
			<br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://computerscience.engineering.unt.edu/"><font color="2D61FF">Department of Computer Science and Engineering</font></a>, <a href="https://www.unt.edu/"><font color="#2D61FF">University of North Texas</font></a>
			<!-- <br>
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://datascience.unt.edu/index.html"><font color="2D61FF">Department of Data Science (Affiliated Faculty)</font></a>, <a href="https://www.unt.edu/"><font color="#2D61FF">University of North Texas</font></a> -->
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Office: Room F284, UNT Discovery Park, 3940 N Elm St, Denton, TX 76207
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Email: heng.fan@unt.edu<!-- hefan<img height="13" width="13"
src="personal/at.gif">cs.stonybrook.edu  -->
			<br>
			
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[<a href="personal/Fan-selected-cv.pdf" target="_blank"><font color="#2D61FF">Curriculum Vitae</font></a>]
			[<a href="https://scholar.google.com/citations?user=MVQYJiMAAAAJ" target="https://scholar.google.com/citations?user=MVQYJiMAAAAJ"><font color="#2D61FF">Google Scholar</font></a>]
			[<a href="https://www.linkedin.com/in/heng-fan-64697aaa/" target="https://www.linkedin.com/in/heng-fan-64697aaa/"><font color="#2D61FF">LinkedIn</font></a>]
			[<a href="https://dblp.org/pid/20/10120-1.html" target="https://dblp.org/pid/20/10120-1.html"><font color="#2D61FF">DBLP</font></a>]
			
        </div>
    </div>
	
	<HR>
	
    <!-- <font size="4" color="black"><b>Short Bio</b></font> -->
    
		<font> <b>Short Bio:</b> Heng Fan is currently an Assistant Professor in the <a href="https://computerscience.engineering.unt.edu/"><font color="#2D61FF">Department of Computer Science and Engineering</font></a> at the <a href="https://www.unt.edu/"><font color="#2D61FF">University of North Texas</font></a>. He received his B.S. in Computer Science and Technology from Huazhong Agricultural University in 2013 and Ph.D. (advised by <a href="https://haibinling.github.io/" target="https://haibinling.github.io/"><font><font color="#2D61FF">Professor Haibin Ling</font></font></a>) in Computer Science from Stony Brook University in 2021, respectively. His research interests include computer vision with a particular interest in various video analysis tasks using vision and language, robotics with a focus on robot vision perception, and medical image analysis. He serves or has served as an Associate Editor for <i>Pattern Recognition</i> (PR) and <i>Image and Vision Computing</i> (IVC), and as an Area Chair for conferences including ICCV, NeurIPS, BMVC, WACV, and ICME. 

	<!-- <HR>

	<font size="3" color="black"><b>To prospective students:</b></font> I am looking for <i><b>self-motivated</b></i> <b>PhD</b>/<b>master</b>/<b>undergraduate</b>/<b>TAMS</b> students and <b>(remote) research interns</b> to work on interesting problems in computer and robotic vision. If you are interested, please read <a href="./prospective-students.html" target="_blank"><font color="#2D61FF">HERE</font></a>. -->
	
	<!-- <font size="3" color="black"><b>To prospective students:</b></font>: <font size="3"> I am looking for <b><i>self-motivated</i> PhD students</b> to work on interesting problems in computer and robotic vision, with a focus on <i>general video analysis</i>. Solid background in mathematics and programming is required. If you're interested, please send your CV, transcript(s), and test scores to my e-mail (heng.fan@unt.edu). Publication in related fields is a plus.<br>

	&nbsp; &nbsp;&nbsp; &nbsp;In addition, motivated master and undergraduate students within UNT as well as TAMS high school students are also welcome. If you're interested in conducting research with me, we should talk.</font> -->
	
	<!-- <HR>

		<font size="3" color="red"><b>Call for papers:</b></font> <font size="3"> We're organizing a <a href="https://dl.acm.org/pb-assets/static_journal_pages/jats/pdf/JATS_SI_UAVS-CFP-1695933597020.pdf" target="_blank"><font color="#2D61FF">special issue</font></a> on <i>Advancements in Uncrewed Aerial Vehicles: Perception, Interaction, Security, Ethics, and
			Beyond</i> at ACM Journal on Autonomous Transportation Systems. Submissions on related topics are welcome. -->

	<!-- <font size="3" color="red"><b>Postdoctoral positions:</b></font> <font size="3"> Several postdoctoral openings are available. 
		The general areas of interests include deep learning, spatial intelligence, mobile computing, and robotic coordination. 
		Check out <a href="https://jobs.untsystem.edu/postings/71770" target="_blank"><font><font color="#2D61FF">HERE</font></font></a>.</font>
	
	<HR>  -->
	
	<HR>

	<font size="4" color="black"><b>News</b>&nbsp;<img src="./personal/news.gif" border="0"></font>
    <ul style="margin-top:0px;">
		<li><font style="font-weight:bold; line-height: 120%">2026-02:</font> A paper on multimodal object tracking (<a href="https://arxiv.org/abs/2508.01592" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/code) is accepted to <a href="https://2026.ieee-icra.org/" target="_blank"><font><font color="#2D61FF">ICRA 2026</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2026-01:</font> A paper on spatial-temporal video grounding (<a href="https://arxiv.org/abs/2503.10500" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/JellyYao3000/OmniSTVG" target="_blank"><font><font color="#2D61FF">code-data</font></font></a>) is accepted to <a href="https://iclr.cc/" target="_blank"><font><font color="#2D61FF">ICLR 2026</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2026-01:</font> A paper on object detection (<a href="https://arxiv.org/abs/2509.09085" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/61s61min/IRDFusion" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to Pattern Recognition.</li>
		<li><font style="font-weight:bold; line-height: 120%">2026-01:</font> A paper on LLM hallucination (<a href="https://openreview.net/pdf?id=T9ot1VVtUb" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ResponsibleAILab/Harmful-Factuality-Hallucination" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to Findings of <a href="https://2026.eacl.org/" target="_blank"><font><font color="#2D61FF">EACL 2026</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-12:</font> I will serve as an Associate Editor for Image and Vision Computing (IVC).</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-11:</font> I will be serving as an Area Chair for <a href="https://2026.ieeeicme.org/" target="_blank"><font><font color="#2D61FF">ICME 2026</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-11:</font> A paper on video event boundary detection (<a href="https://arxiv.org/abs/2512.00475" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) is accepted to <a href="https://wacv.thecvf.com/" target="_blank"><font><font color="#2D61FF">WACV 2026</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-10:</font> A paper on planar tracking (<a href="https://arxiv.org/abs/2510.23368" target="_blank"><font color="#2D61FF">pdf</font></a>/<a href="https://huggingface.co/datasets/Ailovejinx/planartrackplus" target="_blank"><font color="#2D61FF">data</font></a>) is accepted to Computer Vision and Image Understanding (CVIU).</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-10:</font> Recognized as the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/8" target="_blank"><font><font color="#2D61FF">World's Top 2% Scientists</font></font></a> by Standford University/Elsevier.
		<li><font style="font-weight:bold; line-height: 120%">2025-09:</font> Three papers (<a href="https://arxiv.org/abs/2510.11417" target="_blank"><font color="#2D61FF">pdf</font></a>/<a href="https://github.com/juneyeeHu/LM-EEC" target="_blank"><font color="#2D61FF">code</font></a>, <a href="https://openreview.net/pdf?id=q06YjUj0FB" target="_blank"><font color="#2D61FF">pdf</font></a>/<a href="https://github.com/LitingLin/LoRATv2" target="_blank"><font color="#2D61FF">code</font></a>, <a href="https://arxiv.org/abs/2510.16670" target="_blank"><font color="#2D61FF">pdf</font></a>) are accepted to <a href="https://neurips.cc/" target="_blank"><font><font color="#2D61FF">NeurIPS 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-08:</font> A paper on privacy-preserving LLM (<a href="https://arxiv.org/abs/2503.04990" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/FatShion-FTD/DP-GTR" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to Findings of <a href="https://2025.emnlp.org/" target="_blank"><font><font color="#2D61FF">EMNLP 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-07:</font> I will be serving as an Area Chair for <a href="https://wacv.thecvf.com/" target="_blank"><font><font color="#2D61FF">WACV 2026</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-06:</font> Three papers (<a href="https://arxiv.org/abs/2502.07707" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/fb-reps/PRVQL" target="_blank"><font><font color="#2D61FF">code</font></font></a>, <a href="https://arxiv.org/abs/2412.02129" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ailovejinx/GSOT3D" target="_blank"><font><font color="#2D61FF">code-data</font></font></a>, <a href="https://arxiv.org/abs/2503.08145" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/Nathan-Li123/TRACT" target="_blank"><font><font color="#2D61FF">code</font></font></a>) are accepted to <a href="https://iccv.thecvf.com/" target="_blank"><font><font color="#2D61FF">ICCV 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-06:</font> A paper on efficient medical image segmentation (<a href="./publication/MICCAI-25-HRViT.pdf" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/guoyh6/hrvit" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to <a href="https://conferences.miccai.org/2025/en/" target="_blank"><font><font color="#2D61FF">MICCAI 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-06:</font> Two papers on efficient tracking (<a href="https://arxiv.org/abs/2405.17660" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ShaohuaDong2021/LoReTrack" target="_blank"><font><font color="#2D61FF">code</font></font></a>) and action recognition (<a href="https://arxiv.org/abs/2509.07335" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/CyanSea123/G3CN-Gaussian-Topology" target="_blank"><font><font color="#2D61FF">code</font></font></a>) are accepted to <a href="https://www.iros25.org/" target="_blank"><font><font color="#2D61FF">IROS 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-05:</font> I will be serving as an Area Chair for <a href="https://bmvc2025.bmva.org/" target="_blank"><font><font color="#2D61FF">BMVC 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-05:</font> A paper on high-fidelity image inpainting (<a href="https://arxiv.org/abs/2504.12844" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) is accepted to IJCV.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-05:</font> A paper on efficient large language model (<a href="https://arxiv.org/abs/2506.11104" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ResponsibleAILab/DAM" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to Findings of <a href="https://2025.aclweb.org/" target="_blank"><font><font color="#2D61FF">ACL 2025</font></font></a>.</li>
		<!-- <li><font style="font-weight:bold; line-height: 120%">2025-04:</font> Invited <a href="https://www.yapengtian.com/t/6384S25/" target="_blank"><font><font color="#2D61FF">guest lecture</font></font></a> at UT Dallas on visual object tracking.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-04:</font> I will be serving as an Area Chair for <a href="https://neurips.cc/" target="_blank"><font><font color="#2D61FF">NeurIPS 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-03:</font> A paper on multi-view 3D object detection (<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xue_CorrBEV_Multi-View_3D_Object_Detection_by_Correlation_Learning_with_Multi-modal_CVPR_2025_paper.html" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) is accepted to <a href="https://cvpr.thecvf.com/" target="_blank"><font><font color="#2D61FF">CVPR 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-02:</font> I will serve as an Associate Editor for Pattern Recognition (PR).</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-02:</font> Three papers on tracking (<a href="./publication/ICRA_2025_CGTrack.pdf" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/Nightwatch-Fox11/CGTrack" target="_blank"><font><font color="#2D61FF">code</font></font></a>, <a href="https://arxiv.org/pdf/2406.08324" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/Nathan-Li123/LaMOT" target="_blank"><font><font color="#2D61FF">code</font></font></a>) and 3D detection (<a href="./publication/icra25-3d-detectopn.pdf" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) are accepted to <a href="https://2025.ieee-icra.org/" target="_blank"><font><font color="#2D61FF">ICRA 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2025-01:</font> An <b>oral</b> paper on spatio-temporal video grounding (<a href="https://arxiv.org/abs/2502.11168" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/TA-STVG" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to <a href="https://iclr.cc/" target="_blank"><font><font color="#2D61FF">ICLR 2025</font></font></a>.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2024-12:</font> I will be serving as an Area Chair for <a href="https://iccv.thecvf.com/" target="_blank"><font><font color="#2D61FF">ICCV 2025</font></font></a> and <a href="https://2025.ieeeicme.org/" target="_blank"><font><font color="#2D61FF">ICME 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-12:</font> Check out our GSOT3D (<a href="https://arxiv.org/abs/2412.02129" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/code-data) for generic 3D object tracking.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-10:</font> Our VastTrack (<a href="https://arxiv.org/abs/2403.03493" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/VastTrack" target="_blank"><font><font color="#2D61FF">code-data</font></font></a>) for vast category visual tracking is accepted to <a href="https://neurips.cc/" target="_blank"><font><font color="#2D61FF">NeurIPS 2024</font></font></a>.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2024-09:</font> Recognized as the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7" target="_blank"><font><font color="#2D61FF">World's Top 2% Scientists</font></font></a> in 2023 by Stanford University/Elsevier.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-09:</font> Our work TransFlow on optical flow (<a href="./publication/PAMI_TransFlow.pdf" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) is accepted to PAMI.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-07:</font> A paper on multi-view 3D object detection and tracking (<a href="https://arxiv.org/abs/2407.03240" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) is accepted to IJCV.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-07:</font> Two papers on object tracking (<a href="https://arxiv.org/abs/2403.05021" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/SMOT" target="_blank"><font><font color="#2D61FF">code</font></font></a>, <a href="https://arxiv.org/abs/2403.05231" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/LitingLin/LoRAT" target="_blank"><font color="#2D61FF">code</font></a>) are accepted to <a href="https://eccv2024.ecva.net/" target="_blank"><font><font color="#2D61FF">ECCV 2024</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-07:</font> Two <b>oral</b> papers on efficient segmentation (<a href="https://arxiv.org/abs/2312.00360" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ShaohuaDong2021/DPLNet" target="_blank"><font><font color="#2D61FF">code</font></font></a>) and 3D detection (<a href="https://arxiv.org/abs/2312.04822" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/DarrenQu/SiCP" target="_blank"><font><font color="#2D61FF">code</font></font></a>) are accepted to <a href="https://iros2024-abudhabi.org/" target="_blank"><font><font color="#2D61FF">IROS 2024</font></font></a>.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2024-06:</font> A paper on domain adaptive object detection (<a href="https://arxiv.org/abs/2301.00371" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/tiankongzhang/MGA" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to PAMI.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-06:</font> I will serve as an Area Chair for <a href="https://wacv2025.thecvf.com/" target="_blank"><font><font color="#2D61FF">WACV 2025</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-05:</font> A paper on vision-language tracking (<a href="https://arxiv.org/abs/2307.10046" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/JudasDie/SOTS" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to PAMI.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2024-03:</font> Our AttMOT (<a href="https://arxiv.org/abs/2308.07537" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/AttMOT" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to IEEE Transactions on Neural Networks and Learning Systems.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-02:</font> Three papers (<a href="https://arxiv.org/abs/2401.01578" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/CGSTVG" target="_blank"><font><font color="#2D61FF">code</font></font></a>, <a href="https://arxiv.org/abs/2406.04999" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>, <a href="./publication/Text_detection_CVPR_2024_paper.pdf" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) are accepted to <a href="https://cvpr.thecvf.com/" target="_blank"><font><font color="#2D61FF">CVPR 2024</font></font></a>.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2024-01:</font> Welcome PhD student Bing Fan to join our team.</li>
		<li><font style="font-weight:bold; line-height: 120%">2024-01:</font> Our MaGIC for multimodal guided image completion (<a href="https://arxiv.org/abs/2305.11818" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://www.yongshengyu.com/MaGIC-Page/" target="_blank"><font><font color="#2D61FF">project</font></font></a>/<a href="https://github.com/yeates/MaGIC" target="_blank"><font><font color="#2D61FF">code</font></font></a>) is accepted to <a href="https://iclr.cc/" target="_blank"><font><font color="#2D61FF">ICLR 2024</font></font></a>.</li> -->
		<!-- 
		<li><font style="font-weight:bold; line-height: 120%">2024-01:</font> Check out our CG-STVG (<a href="https://arxiv.org/abs/2401.01578" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/HengLan/CGSTVG" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on spatio-temporal video grounding.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-12:</font> Check out our FGDVI (<a href="https://arxiv.org/abs/2311.15368" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/NevSNev/FGDVI" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on flow-guided diffusion for video inpainting.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-12:</font> Check out our DPLNet (<a href="https://arxiv.org/abs/2312.00360" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ShaohuaDong2021/DPLNet" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on efficient multimodal semantic segmentation.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2023-12:</font> A paper (<a href="https://arxiv.org/abs/2312.06049" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>) on pedestrian attribute recognition is accepted to Pattern Recognition (PR).</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2023-12:</font> Check out our DPLNet (<a href="https://arxiv.org/abs/2312.00360" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/ShaohuaDong2021/DPLNet" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on efficient multimodal semantic segmentation.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-12:</font> Check out our FGDVI (<a href="https://arxiv.org/abs/2311.15368" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/NevSNev/FGDVI" target="_blank"><font><font color="#2D61FF">code</font></font></a>) for flow-guided diffusion for video inpainting.</li> -->
		<!-- <li><font style="font-weight:bold; line-height: 120%">2023-10:</font> Recognized as the World's top 2% scientists in 2022 by Standford University.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-09:</font> A paper (<a href="https://arxiv.org/abs/2309.15431" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/GX77/LCVSL" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on event boundary detection from video stream is accepted to IJCV.</li> -->

		<!--
		<li><font style="font-weight:bold; line-height: 120%">2023-09:</font> A paper (<a href="https://sigspatial.yunhefeng.me/" target="_blank"><font><font color="#2D61FF">project</font></font></a>) on spatial computing is accepted to <a href="https://sigspatial2023.sigspatial.org/" target="_blank"><font><font color="#2D61FF">SIGSPATIAL 2023</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-08:</font> A paper (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323006118" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/chanchanchan97/ICAFusion" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on RGB-Thermal object detection is accepted to Pattern Recognition (PR).</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-08:</font> Our PIDray (<a href="https://arxiv.org/abs/2211.10763" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/lutao2021/PIDray" target="_blank"><font><font color="#2D61FF">code-data</font></font></a>) on prohibited item detection is accepted to IJCV.</li>

		
		<li><font style="font-weight:bold; line-height: 120%">2023-08:</font> Check out our DMT (<a href="https://arxiv.org/abs/2307.08629" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/yeates/DMT" target="_blank"><font><font color="#2D61FF">code</font></font></a>) for state-of-the-art video inpainting.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-07:</font> A paper (<a href="./publication/CVIU-COST-23.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>/<a href="https://github.com/wanghao14/COST" target="_blank"><font><font color="#2D61FF">code</font></font></a>) on video captioning is accepted to Computer Vision and Image Understanding (CVIU).</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-07:</font> Four papers (<a href="https://arxiv.org/pdf/2308.08182.pdf" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/tiankongzhang/NSA" target="_blank"><font><font color="#2D61FF">code</font></font></a>,  
			<a href="https://arxiv.org/abs/2304.11335" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/NevSNev/UniST" target="_blank"><font><font color="#2D61FF">code</font></font></a>, 
			<a href="https://arxiv.org/abs/2309.12867" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/acherstyx/CoCap" target="_blank"><font><font color="#2D61FF">code</font></font></a>,
			<a href="https://arxiv.org/abs/2303.07625" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://hengfan2010.github.io/projects/PlanarTrack/" target="_blank"><font><font color="#2D61FF">code-data</font></font></a>) are accepted to <a href="https://iccv2023.thecvf.com/" target="_blank"><font><font color="#2D61FF">ICCV 2023</font></font></a>.</li>
		
		<li><font style="font-weight:bold; line-height: 120%">2023-05:</font> Appointed as an Area Chair for <a href="https://wacv2024.thecvf.com/" target="_blank"><font><font color="#2D61FF">WACV 2024</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-05:</font> Release MaGIC (<a href="https://arxiv.org/abs/2305.11818" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://github.com/yeates/MaGIC" target="_blank"><font><font color="#2D61FF">code</font></font></a>) for multi-modality guided image completion and generation.</li>

		
		<li><font style="font-weight:bold; line-height: 120%">2023-03:</font> Check out PlanarTrack (<a href="https://arxiv.org/abs/2303.07625" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://hengfan2010.github.io/projects/PlanarTrack/" target="_blank"><font><font color="#2D61FF">project</font></font></a>) for large-scale challenging planar object tracking.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-02:</font> A paper on visual object tracking is accepted to The Innovation.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-01:</font> Welcome PhD student Shaohua Dong to join our team.</li>
		<li><font style="font-weight:bold; line-height: 120%">2023-01:</font> Check out our new preprint (<a href="https://arxiv.org/abs/2301.00371" target="_blank"><font><font color="#2D61FF">arXiv</font></font></a>) on domain adaptive object detection.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-11:</font> Our AnimalTrack (<a href="https://arxiv.org/abs/2205.00158" target="_blank"><font><font color="#2D61FF">pdf</font></font></a>/<a href="https://hengfan2010.github.io/projects/AnimalTrack/" target="_blank"><font><font color="#2D61FF">project</font></font></a>) is accepted to IJCV.</li>
		
		

		<li><font style="font-weight:bold; line-height: 120%">2022-11:</font> Recognized as the <b>World's Top 2% Scientists</b> (year 2021) by Stanford University. Check <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/3" target="_blank"><font><font color="#2D61FF">here</font></font></a> or <a href="https://research.unt.edu/news/unt-faculty-named-among-world%E2%80%99s-most-cited-researchers" target="_blank"><font><font color="#2D61FF">UNT News</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-11:</font> Congrats to Xiaoqiong for her first paper accepted to <a href="https://www.mmm2023.no/" target="_blank"><font><font color="#2D61FF">MMM 2023</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-09:</font> Two papers, SwinTrack (<a href="./publication/SwinTrack-NeurIPS-2022.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>, <a href="https://github.com/litinglin/swintrack" target="_blank"><font color="#2D61FF">code</font></a>) and VLT (<a href="./publication/VLT-NeurIPS-2022.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>, <a href="https://github.com/JudasDie/SOTS" target="_blank"><font color="#2D61FF">code</font></a>), on visual tracking are accepted to <a href="https://nips.cc/" target="_blank"><font><font color="#2D61FF">NeurIPS 2022</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-09:</font> A paper on domain distribution alignment (<a href="./publication/MedIA-2022.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>) is accepted to MedIA.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-07:</font> Work on image inpainting (<a href="./publication/InvertFill-ECCV-2022.pdf" target="_blank"><font color="#2D61FF">pdf</font></a>) is accepted to <a href="https://eccv2022.ecva.net/" target="_blank"><font><font color="#2D61FF">ECCV 2022</font></font></a>.</li>
		
		<li><font style="font-weight:bold; line-height: 120%">2022-05:</font> Serve as an Area Chair for <a href="https://wacv2023.thecvf.com/" target="_blank"><font><font color="#2D61FF">WACV 2023</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-05:</font> Release AnimalTrack (<a href="https://arxiv.org/abs/2205.00158" target="_blank"><font><font color="#2D61FF">arXiv</font></font></a>) for multi-animal tracking. Stay tuned for updates.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-04:</font> Our work <a href="https://arxiv.org/abs/2201.02526" target="_blank"><font><font color="#2D61FF">InBN</font></font></a> (<a href="https://arxiv.org/abs/2201.02526" target="_blank"><font color="#2D61FF">pdf</font></a>, <a href="https://github.com/JudasDie/SOTS" target="_blank"><font color="#2D61FF">code</font></a>) on tracking is accepted to <a href="https://www.ijcai.org/" target="_blank"><font><font color="#2D61FF"> IJCAI 2022</font></font></a> as <b>Long Oral</b>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-01:</font> Check <a href="https://arxiv.org/abs/2201.02526" target="_blank"><font><font color="#2D61FF">InBN</font></font></a> on arXiv, a simple, general and effective backbone for improving tracking.</li>
		<li><font style="font-weight:bold; line-height: 120%">2022-01:</font> Welcome PhD student Xiaoqiong Liu to join our team.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-11:</font> A paper on image generation accepted to Pattern Recognition.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-10:</font> A paper on UAV tracking and detection accepted to PAMI.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-08:</font> TOTB has been released, go check it out <a href="https://hengfan2010.github.io/projects/TOTB/" target="_blank"><font><font color="#2D61FF"> here</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-07:</font> A paper on transparent object tracking accepted to <a href="http://iccv2021.thecvf.com/home" target="_blank"><font><font color="#2D61FF"> ICCV 2021</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-07:</font> I will serve as an Area Chair for <a href="http://wacv2022.thecvf.com/home"><font><font color="#2D61FF"> WACV 2022</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-07:</font> We have one paper on visual object tracking accepted to <a href="https://www.iros2021.org/"><font><font color="#2D61FF"> IROS 2021</font></font></a>.</li>
		<li><font style="font-weight:bold; line-height: 120%">2021-05:</font> I passed my PhD dissertation. So much gratitude for everyone I met along this journey.</li> 
		-->
    </ul>
	
	
	<HR>
	
	<font size="4" color="black"><b>Quick Links</b></font> <font size="3"> 
		<br>
		&nbsp; &nbsp; &nbsp;
		<a href="https://my.unt.edu/psp/ps/?cmd=login" target="_blank"><font color="#2D61FF" style="line-height: 150%;">myUNT</font></a> &nbsp;   
		<a href="https://it.unt.edu/eagleconnect" target="_blank"><font color="#2D61FF" style="line-height: 150%;">EagleConnect</font></a> &nbsp;   
		<a href="https://computerscience.engineering.unt.edu/" target="_blank"><font color="#2D61FF">UNT CSE</font></a>   &nbsp; 
		<a href="https://www.unt.edu/find-people-departments" target="_blank"><font color="#2D61FF">UNT Directory</font></a>   &nbsp; 
		<a href="https://transportation.unt.edu/sites/default/files/unt_campus_parking.pdf" target="_blank"><font color="#2D61FF">UNT Map</font></a>   &nbsp; 
		<a href="https://www.google.com/" target="_blank"><font color="#2D61FF">Google</font></a>     &nbsp; 
		<a href="https://scholar.google.com/" target="_blank"><font color="#2D61FF">gScholar</font></a>   &nbsp; 
		<!-- <a href="http://maps.google.com/" target="_blank"><font color="#2D61FF">gMap</font></a>   &nbsp;  -->
		<a href="https://www.thecvf.com/" target="_blank"><font color="#2D61FF">CVF</font></a>   &nbsp; 
		<a href="https://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank"><font color="#2D61FF">IEEE</font></a> &nbsp; 
		<a href="https://csrankings.org/#/index?all&us" target="_blank"><font color="#2D61FF">CSRankings</font></a> &nbsp; 
		<a href="https://www.editorialmanager.com/pr/default.aspx?adobe_mc=MCMID%3D29504155241840331153107436488397987575%7CMCORGID%3D4D6368F454EC41940A4C98A6%2540AdobeOrg%7CTS%3D1740507429" target="_blank"><font color="#2D61FF">PR</font></a> &nbsp; 
		<a href="https://www.editorialmanager.com/imavis/Default.aspx" target="_blank"><font color="#2D61FF">IVC</font></a>
	
	<HR>
	
	
</div>

</body></html>
